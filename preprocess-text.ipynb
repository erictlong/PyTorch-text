{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prprocess custom text data using Torchtext\n",
    "\n",
    "Illustrates the usage of torchtext on a dataset that is not built-in. We will preprocess a dataset that can be further utilized to train a sequeence to sequence model for machine translation without using legacy version of torchtext.\n",
    "\n",
    "The notebook will cover:\n",
    "- read a dataset\n",
    "- Tokenize sentence\n",
    "- apply transformation to sentence\n",
    "- perform bucketing batching\n",
    "\n",
    "Link: https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'deu.txt'\n",
    "data_pip = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pip = dp.iter.FileOpener(data_pip, mode='rb')\n",
    "data_pip = data_pip.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code block, we are doing following things:\n",
    "1. We are creating an iterable of filenames\n",
    "2. We pass the iterable to fileopener which then opens the files in read mode\n",
    "3. We call a function to parse the file, which again returns an iterable of tuples representing each rows of the tab delimited file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify if the iterable has the pair of sentences as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pip:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we also have attribution details along with pairs of sentences. We will write a small function to remove the attribution details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"Function to keep the first element in a tumple\"\"\"\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pip = data_pip.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the data_pipe only contains pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pip:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a few function to perform tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"Tokenize an english text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"Tokenize a german text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the functions above to confirm they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'a', 'good', 'date', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"have a good date!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haben', 'sie', 'einen', 'guten', 'tag', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(deTokenize(\"haben sie einen guten tag!!!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
