{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prprocess custom text data using Torchtext\n",
    "\n",
    "Illustrates the usage of torchtext on a dataset that is not built-in. We will preprocess a dataset that can be further utilized to train a sequeence to sequence model for machine translation without using legacy version of torchtext.\n",
    "\n",
    "The notebook will cover:\n",
    "- read a dataset\n",
    "- Tokenize sentence\n",
    "- apply transformation to sentence\n",
    "- perform bucketing batching\n",
    "\n",
    "Link: https://pytorch.org/tutorials/beginner/torchtext_custom_dataset_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata.datapipes as dp\n",
    "import torchtext.transforms as T\n",
    "import spacy\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = spacy.load(\"en_core_web_sm\")\n",
    "de = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'deu.txt'\n",
    "data_pip = dp.iter.IterableWrapper([FILE_PATH])\n",
    "data_pip = dp.iter.FileOpener(data_pip, mode='rb')\n",
    "data_pip = data_pip.parse_csv(skip_lines=0, delimiter='\\t', as_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code block, we are doing following things:\n",
    "1. We are creating an iterable of filenames\n",
    "2. We pass the iterable to fileopener which then opens the files in read mode\n",
    "3. We call a function to parse the file, which again returns an iterable of tuples representing each rows of the tab delimited file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify if the iterable has the pair of sentences as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.', 'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8597805 (Roujin)')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pip:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that we also have attribution details along with pairs of sentences. We will write a small function to remove the attribution details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeAttribution(row):\n",
    "    \"\"\"Function to keep the first element in a tumple\"\"\"\n",
    "    return row[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pip = data_pip.map(removeAttribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verify the data_pipe only contains pair of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Go.', 'Geh.')\n"
     ]
    }
   ],
   "source": [
    "for sample in data_pip:\n",
    "    print(sample)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define a few function to perform tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engTokenize(text):\n",
    "    \"\"\"Tokenize an english text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in eng.tokenizer(text)]\n",
    "\n",
    "def deTokenize(text):\n",
    "    \"\"\"Tokenize a german text and return a list of tokens\"\"\"\n",
    "    return [token.text for token in de.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out the functions above to confirm they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'a', 'good', 'date', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(engTokenize(\"have a good date!!!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haben', 'sie', 'einen', 'guten', 'tag', '!', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "print(deTokenize(\"haben sie einen guten tag!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Voacbulary\n",
    "\n",
    "Lets us consider an english sentence as the source and german sentence as the target.\n",
    "\n",
    "Vocabulary can be considered as the set of unique words we have in the dataset. We will build vocabulary for both our source and target now.\n",
    "\n",
    "We neeed to define a function to get tokens from elements of tuples in the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(data_iter, place):\n",
    "    \"\"\"\n",
    "    Function to yield tokens from an iterator. Since our iterator contains tuples of sentences\n",
    "    (sorce and target), \"place\" parameters defines for which index to return the token for. \n",
    "    \"place = 0\" for source and \"place=1\" for traget\n",
    "    \"\"\"\n",
    "    for english, german in data_iter:\n",
    "        if place == 0:\n",
    "            yield english(english)\n",
    "        else:\n",
    "            yield deTokenize(german)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build vocabulary for source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pip, 2),\n",
    "    min_freq=2,\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "source_vocab.set_default_index(source_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above builds the vocabulary from the iterator:\n",
    "1. getokens function places=0 as we need vocabulary for source sentences\n",
    "2. set min_feq=2 means the function will skip those words that occurs less than 2 times\n",
    "3. We specify some special tokens\n",
    "    - sos for the start of sentence\n",
    "    - eos for the end of sentence\n",
    "    - unk for unknown words\n",
    "    - pad is padding token\n",
    "4. set special_first=true which means pad will get index 0, sos will be index 1, eos index 2 and unk will get index 3 in the vocabulary\n",
    "5. set the default index as index of unk. If some words is not in the vocabulary we will use unk instead of that unknown word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will build vocabulary for target sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = build_vocab_from_iterator(\n",
    "    getTokens(data_pip, 1),\n",
    "    min_freq=2,\n",
    "    specials=['<pad>', '<sos>', '<eos>', '<unk>'],\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "target_vocab.set_default_index(target_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example above shows how we can add special tokens to our vocabulary. Special tokens may change based on the requirements.\n",
    "\n",
    "Now we can verify that special tokens are placed at the beginning and then othe words. in the below code reutrns a list which tokens at index based on vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', '.', ',', 'Tom', 'Ich', '?']\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab.get_itos()[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalize sentence using vocabulary\n",
    "\n",
    "After building the vocabulary, we need to convert our sentences to corresponding indcies.\n",
    "\n",
    "We will need additional functions for this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
